#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
from datetime import datetime, timedelta

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models.variable import Variable

from utils.xhs_appium import XHSOperator
from utils.wechat_channl import send_wx_msg


def collect_xhs_notes(**context) -> None:
    """
    æ”¶é›†å°çº¢ä¹¦ç¬”è®°
    
    ä»å°çº¢ä¹¦æœç´¢æŒ‡å®šå…³é”®è¯çš„ç¬”è®°å¹¶é€šè¿‡xcomä¼ é€’ç»™ä¸‹ä¸€ä¸ªä»»åŠ¡ã€‚
    
    Args:
        **context: Airflowä¸Šä¸‹æ–‡å‚æ•°å­—å…¸
    
    Returns:
        None
    """
    # è·å–è§¦å‘æ¶ˆæ¯çš„å¾®ä¿¡ç¾¤IDå’Œæ¥æºIP
    message_data = context['dag_run'].conf.get('current_message', {})
    group_id = message_data.get('roomid')
    source_ip = message_data.get('source_ip')
    
    # è·å–å…³é”®è¯ï¼Œé»˜è®¤ä¸º"AIå®¢æœ"
    keyword = (context['dag_run'].conf.get('keyword', 'æ·±åœ³ç½‘çƒ') 
              if context['dag_run'].conf 
              else 'æ·±åœ³ç½‘çƒ')
    
    # å‘é€å¼€å§‹æœç´¢çš„æé†’
    if group_id and source_ip:
        send_wx_msg(
            wcf_ip=source_ip,
            message=f"ğŸ” æ­£åœ¨æœç´¢ã€Œ{keyword}ã€çš„æœ€æ–°çœŸå®ç¬”è®°ï¼Œè¯·ç¨ç­‰~",
            receiver=group_id
        )

    # è·å–æœ€å¤§æ”¶é›†ç¬”è®°æ•°ï¼Œé»˜è®¤ä¸º5
    max_notes = (context['dag_run'].conf.get('max_notes', 10)
                if context['dag_run'].conf
                else 10)
    
    # è·å–AppiumæœåŠ¡å™¨URL
    appium_server_url = Variable.get("APPIUM_SERVER_URL", "http://localhost:4723")
    
    print(f"å¼€å§‹æ”¶é›†å…³é”®è¯ '{keyword}' çš„å°çº¢ä¹¦ç¬”è®°...")
    
    try:
        # åˆå§‹åŒ–å°çº¢ä¹¦æ“ä½œå™¨
        xhs = XHSOperator(appium_server_url=appium_server_url, force_app_launch=True)
        
        # # æ£€æŸ¥æ˜¯å¦åœ¨é¦–é¡µ
        # if not xhs.is_at_xhs_home_page():
        #     xhs.return_to_home_page()
        
        # æ”¶é›†ç¬”è®°
        notes = xhs.collect_notes_by_keyword(
            keyword=keyword,
            max_notes=max_notes,
            filters={'sort_by': 'æœ€æ–°', 'note_type': 'å›¾æ–‡'},
            include_comments=False
        )
        
        if not notes:
            print(f"æœªæ‰¾åˆ°å…³äº '{keyword}' çš„ç¬”è®°")
            return
            
        # è¿‡æ»¤æ ‡ç­¾
        for note in notes:
            if 'content' in note:
                # å°†å†…å®¹æŒ‰ç©ºæ ¼åˆ†å‰²ï¼Œè¿‡æ»¤æ‰ä»¥#å¼€å¤´çš„éƒ¨åˆ†
                content = note['content']
                words = content.split()
                filtered_words = [word for word in words if not word.startswith('#')]
                # é‡æ–°ç»„åˆå†…å®¹
                note['content'] = ' '.join(filtered_words)
        
        # æ‰“å°æ”¶é›†ç»“æœ
        print("\næ”¶é›†å®Œæˆ!")
        print(f"å…±æ”¶é›†åˆ° {len(notes)} æ¡ç¬”è®°:")
        for note in notes:
            print(note)

        # ä½¿ç”¨å›ºå®šçš„XCom key
        task_instance = context['task_instance']
        task_instance.xcom_push(key="collected_notes", value=notes)
            
    except Exception as e:
        error_msg = f"æ”¶é›†å°çº¢ä¹¦ç¬”è®°å¤±è´¥: {str(e)}"
        print(error_msg)
        raise
    finally:
        # ç¡®ä¿å…³é—­å°çº¢ä¹¦æ“ä½œå™¨
        if 'xhs' in locals():
            xhs.close()


def classify_notes_by_llm(**context) -> None:
    """
    ä½¿ç”¨å¤§æ¨¡å‹å¯¹æ”¶é›†çš„å°çº¢ä¹¦ç¬”è®°è¿›è¡Œåˆ†ç±»
    
    Args:
        **context: Airflowä¸Šä¸‹æ–‡å‚æ•°å­—å…¸
    
    Returns:
        None
    """
    from utils.llm_channl import get_llm_response
    
    # è·å–å…³é”®è¯
    keyword = (context['dag_run'].conf.get('keyword', 'æ·±åœ³ç½‘çƒ') 
              if context['dag_run'].conf 
              else 'æ·±åœ³ç½‘çƒ')
    
    # ä»XComè·å–ç¬”è®°æ•°æ®ï¼Œä½¿ç”¨å›ºå®škey
    task_instance = context['task_instance']
    notes = task_instance.xcom_pull(
        task_ids='collect_xhs_notes',
        key="collected_notes"
    )
    
    if not notes:
        print(f"æœªæ‰¾åˆ°å…³äº '{keyword}' çš„ç¼“å­˜ç¬”è®°")
        return
        
    # æ„å»ºç³»ç»Ÿæç¤ºè¯
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¤¾äº¤åª’ä½“å†…å®¹åˆ†æä¸“å®¶ã€‚è¯·å¸®æˆ‘åˆ†æå°çº¢ä¹¦ç¬”è®°çš„æ€§è´¨ï¼Œå°†å…¶åˆ†ä¸ºä»¥ä¸‹ä¸¤ç±»ï¼š
    1. è¥é”€ç¬”è®°ï¼š
       - æ˜æ˜¾çš„å¹¿å‘Š/æ¨é”€æ„å›¾
       - è¿‡åˆ†å¤¸å¼ çš„æ•ˆæœæè¿°
       - å¤§é‡è¥é”€å…³é”®è¯(å¦‚"ä¸“æŸœä»·"ã€"ä¼˜æƒ "ã€"æ¨è"ç­‰)
       - è™šå‡æˆ–å¤¸å¤§çš„ç”¨æˆ·ä½“éªŒ
       - å¸¦æœ‰æ˜æ˜¾çš„å¯¼æµæˆ–å¼•å¯¼è´­ä¹°æ„å›¾
    
    2. çœŸå®ç¬”è®°ï¼š
       - çœŸå®çš„ä¸ªäººä½“éªŒåˆ†äº«
       - å®¢è§‚çš„æè¿°å’Œè¯„ä»·
       - åŒ…å«å…·ä½“ç»†èŠ‚å’ŒçœŸå®åœºæ™¯
       - æœ‰çœŸå®çš„æƒ…æ„Ÿè¡¨è¾¾
       - å¯èƒ½åŒ…å«ä¼˜ç¼ºç‚¹çš„å¹³è¡¡è¯„ä»·
    
    è¯·ä»”ç»†åˆ†æç¬”è®°çš„å†…å®¹ã€è¯­æ°”å’Œè¡¨è¾¾æ–¹å¼ï¼Œç»™å‡ºåˆ†ç±»ç»“æœå’Œç®€è¦åˆ†æç†ç”±ã€‚
    
    è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
    {
        "category": "è¥é”€ç¬”è®°/çœŸå®ç¬”è®°",
        "reason": "åˆ†ç±»ç†ç”±",
        "confidence": "é«˜/ä¸­/ä½"
    }
    """
    
    classified_results = []
    
    # é€æ¡åˆ†æç¬”è®°
    print(f"å¼€å§‹åˆ†æ {len(notes)} æ¡ç¬”è®°")
    for note in notes:
        try:
            # æ„å»ºé—®é¢˜
            question = f"""è¯·åˆ†æè¿™æ¡å°çº¢ä¹¦ç¬”è®°çš„æ€§è´¨ï¼š
            æ ‡é¢˜ï¼š{note.get('title', '')}
            å†…å®¹ï¼š{note.get('content', '')}
            """
            
            # è°ƒç”¨å¤§æ¨¡å‹
            response = get_llm_response(
                user_question=question,
                system_prompt=system_prompt,
                model_name="gpt-4o-mini"
            )
            
            # è§£æå“åº”
            result = json.loads(response)
            result['note'] = note
            classified_results.append(result)
            
        except Exception as e:
            print(f"åˆ†æç¬”è®°å¤±è´¥: {str(e)}")
            continue
    
    # ä½¿ç”¨å›ºå®šçš„XCom keyä¼ é€’åˆ†ç±»ç»“æœ
    task_instance.xcom_push(
        key="classified_notes",
        value=classified_results
    )
    
    # æ‰“å°åˆ†ç±»ç»“æœç»Ÿè®¡
    marketing_count = sum(1 for r in classified_results if r['category'] == 'è¥é”€ç¬”è®°')
    genuine_count = sum(1 for r in classified_results if r['category'] == 'çœŸå®ç¬”è®°')
    print(f"\nåˆ†ç±»å®Œæˆï¼å…±åˆ†æ {len(classified_results)} æ¡ç¬”è®°:")
    print(f"è¥é”€ç¬”è®°: {marketing_count} æ¡")
    print(f"çœŸå®ç¬”è®°: {genuine_count} æ¡")

def summarize_notes(**context) -> None:
    """
    æ€»ç»“å’Œæ‘˜è¦çœŸå®çš„ç¬”è®°ä¿¡æ¯ï¼Œå¹¶æ¨é€åˆ°å¾®ä¿¡ç¾¤ä¸­
    æ¯ä¸ªç¬”è®°ç”¨ä¸€å¥è¯æ€»ç»“ï¼Œåˆ†æ‰¹å¤„ç†åæ±‡æ€»å±•ç¤º
    
    Args:
        **context: Airflowä¸Šä¸‹æ–‡å‚æ•°å­—å…¸
    
    Returns:
        None
    """
    from utils.llm_channl import get_llm_response
    from utils.wechat_channl import send_wx_msg
    
    # è·å–å…³é”®è¯
    keyword = (context['dag_run'].conf.get('keyword', 'æ·±åœ³ç½‘çƒ') 
              if context['dag_run'].conf 
              else 'æ·±åœ³ç½‘çƒ')
    
    # è·å–è§¦å‘æ¶ˆæ¯çš„å¾®ä¿¡ç¾¤IDå’Œæ¥æºIP
    message_data = context['dag_run'].conf.get('current_message', {})
    group_id = message_data.get('roomid')
    source_ip = message_data.get('source_ip')
    
    if not group_id or not source_ip:
        print("æœªæä¾›å¾®ä¿¡ç¾¤IDæˆ–æ¥æºIPï¼Œæ— æ³•æ¨é€æ¶ˆæ¯")
        return
    
    # ä»XComè·å–åˆ†ç±»ç»“æœï¼Œä½¿ç”¨å›ºå®škey
    task_instance = context['task_instance']
    classified_results = task_instance.xcom_pull(
        task_ids='classify_notes_by_llm',
        key="classified_notes"
    )
    
    if not classified_results:
        print(f"æœªæ‰¾åˆ°å…³äº '{keyword}' çš„åˆ†ç±»ç»“æœ")
        return
    
    # ç­›é€‰çœŸå®ç¬”è®°å’Œè¥é”€ç¬”è®°
    genuine_notes = [
        result['note'] for result in classified_results 
        if result['category'] == 'çœŸå®ç¬”è®°'
    ]
    marketing_notes_count = sum(1 for result in classified_results 
                              if result['category'] == 'è¥é”€ç¬”è®°')
    
    if not genuine_notes:
        return "æœªå‘ç°å…³äº '{keyword}' çš„çœŸå®ç¬”è®°"
    
    BATCH_SIZE = 5  # æ¯æ‰¹å¤„ç†5æ¡ç¬”è®°
    
    # ä¿®æ”¹ç³»ç»Ÿæç¤ºè¯
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹æ€»ç»“ä¸“å®¶ã€‚è¯·ç”¨ä¸€å¥è¯æ€»ç»“æ¯æ¡å°çº¢ä¹¦ç¬”è®°çš„æ ¸å¿ƒå†…å®¹ã€‚
    è¦æ±‚ï¼š
    1. æ¯æ¡ç¬”è®°çš„æ€»ç»“ä¸è¶…è¿‡15ä¸ªå­—
    2. çªå‡ºç¬”è®°çš„å…³é”®ä¿¡æ¯å’Œç‹¬ç‰¹è§è§£
    3. ä¿æŒå®¢è§‚ä¸­ç«‹çš„è¯­æ°”
    4. æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
    {
        "summaries": [
            {"summary": "ä¸€å¥è¯æ€»ç»“", "url": "ç¬”è®°é“¾æ¥"}
        ]
    }
    """
    all_summaries = []
    
    # åˆ†æ‰¹å¤„ç†ç¬”è®°
    for i in range(0, len(genuine_notes), BATCH_SIZE):
        batch_notes = genuine_notes[i:i + BATCH_SIZE]
        
        # æ„å»ºå½“å‰æ‰¹æ¬¡çš„ç¬”è®°å†…å®¹
        notes_content = []
        for note in batch_notes:
            notes_content.append(f"""
æ ‡é¢˜ï¼š{note.get('title', '')}
å†…å®¹ï¼š{note.get('content', '')}
é“¾æ¥ï¼š{note.get('url', 'æœªçŸ¥')}
            """)
        
        # è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå½“å‰æ‰¹æ¬¡çš„æ€»ç»“
        question = f"è¯·åˆ†åˆ«ç”¨ä¸€å¥è¯æ€»ç»“ä»¥ä¸‹{len(notes_content)}æ¡å°çº¢ä¹¦ç¬”è®°ï¼š\n" + "\n---\n".join(notes_content)
        
        summary_response = get_llm_response(
            user_question=question,
            system_prompt=system_prompt,
            model_name="gpt-4o-mini"
        )
        
        # è§£æJSONå“åº”å¹¶åˆå¹¶ç»“æœ
        batch_summary_data = json.loads(summary_response)
        all_summaries.extend(batch_summary_data['summaries'])
        
        # æ‰“å°è¿›åº¦
        print(f"å·²å®Œæˆ {min(i + BATCH_SIZE, len(genuine_notes))}/{len(genuine_notes)} æ¡ç¬”è®°çš„æ€»ç»“")
    
    # æ„å»ºæœ€ç»ˆæ¶ˆæ¯
    message_parts = [
        f"ğŸ” {keyword} | æ€»{len(classified_results)}æ¡ | çœŸå®{len(genuine_notes)}æ¡ | è¥é”€{marketing_notes_count}æ¡\n",
        f" ç¬”è®°æ¦‚è¦ï¼š",
    ]
    
    for idx, item in enumerate(all_summaries, 1):
        message_parts.append(f"{idx}. {item['summary']}")
        message_parts.append(f"   ğŸ‘‰ {item['url']}\n")
    
    message = "\n".join(message_parts)
    
    # ä½¿ç”¨send_wx_msgå‘é€æ¶ˆæ¯
    send_wx_msg(
        wcf_ip=source_ip,
        message=message,
        receiver=group_id
    )
    print(f"å·²å°†æ‰€æœ‰ç¬”è®°æ€»ç»“æ¨é€åˆ°å¾®ä¿¡ç¾¤")
        

# DAG å®šä¹‰
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
}

dag = DAG(
    dag_id='xhs_notes_watcher',
    default_args=default_args,
    description='å°çº¢ä¹¦ç¬”è®°æ”¶é›†å·¡æ£€',
    schedule_interval=None, 
    tags=['å°çº¢ä¹¦'],
    catchup=False,
    concurrency=1,  # é™åˆ¶åŒæ—¶è¿è¡Œçš„ä»»åŠ¡å®ä¾‹æ•°
    max_active_tasks=1,  # é™åˆ¶åŒæ—¶è¿è¡Œçš„ä»»åŠ¡æ•°
    max_active_runs=1,
)

collect_notes_task = PythonOperator(
    task_id='collect_xhs_notes',
    python_callable=collect_xhs_notes,
    provide_context=True,
    retries=2,
    retry_delay=timedelta(seconds=3),
    dag=dag,
)

classify_notes_task = PythonOperator(
    task_id='classify_notes_by_llm',
    python_callable=classify_notes_by_llm,
    provide_context=True,
    dag=dag,
)

publish_analysis_task = PythonOperator(
    task_id='publish_analysis_note',
    python_callable=summarize_notes,
    provide_context=True,
    dag=dag,
)

# è®¾ç½®ä»»åŠ¡ä¾èµ–å…³ç³»
collect_notes_task >> classify_notes_task >> publish_analysis_task
