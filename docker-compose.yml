#===============================================================================
# 基础配置
#===============================================================================
x-common: &common
  restart: unless-stopped
  networks:
    - airflow-network

networks:
  airflow-network:
    driver: bridge

#===============================================================================
# Airflow 基础配置
#===============================================================================
x-airflow-common: &airflow-common
  <<: *common
  image: bitnami/airflow:latest
  volumes: &airflow-volumes
    - ./dags:/opt/bitnami/airflow/dags
    - ./logs:/opt/bitnami/airflow/logs

#===============================================================================
# Airflow 环境变量
#===============================================================================
x-airflow-env: &airflow-env
  # 密钥配置
  AIRFLOW_FERNET_KEY: &fernet_key 46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
  AIRFLOW_SECRET_KEY: &secret_key a25mQ1FHTUh3MnFRSk5KMEIyVVU2YmN0VGRyYTVXY08=
  AIRFLOW_EXECUTOR: CeleryExecutor
  AIRFLOW_USERNAME: ${AIRFLOW_USERNAME}
  AIRFLOW_PASSWORD: ${AIRFLOW_PASSWORD}
  AIRFLOW_EMAIL: ${AIRFLOW_EMAIL}

  # 数据库配置
  AIRFLOW_DATABASE_NAME: &db_name ${AIRFLOW_DATABASE_NAME}
  AIRFLOW_DATABASE_USERNAME: &db_user ${AIRFLOW_DATABASE_USERNAME}
  AIRFLOW_DATABASE_PASSWORD: &db_pass ${AIRFLOW_DATABASE_PASSWORD}

  # 核心配置
  AIRFLOW__CORE__PARALLELISM: 32
  AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: 16
  AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: 8
  AIRFLOW__CORE__DEFAULT_TIMEZONE: Asia/Shanghai
  # 这里使用了环境变量 AIRFLOW_DAGS_FOLDER 来配置 DAG 文件夹路径
  AIRFLOW__CORE__DAGS_FOLDER: ${AIRFLOW_DAGS_FOLDER:-/opt/bitnami/airflow/dags}
  AIRFLOW__CORE__LOAD_EXAMPLES: "false"
  AIRFLOW__CORE__IGNORE_DAGS_ON_LOAD_ERROR: "true"
  AIRFLOW__CORE__DAGS_FOLDER_SKIP_PATTERNS: "example_*.py,examples/*.py"

  # Webserver 配置
  AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
  AIRFLOW__WEBSERVER__ALLOW_CONFIG_TRIGGER: "True"

  # Scheduler 配置
  AIRFLOW__SCHEDULER__STANDALONE_DAG_PROCESSOR: True
  AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 30

  # API 配置
  AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
  AIRFLOW__API__ACCESS_CONTROL_ALLOW_HEADERS: "*"
  AIRFLOW__API__ACCESS_CONTROL_ALLOW_METHODS: "*"
  AIRFLOW__API__ACCESS_CONTROL_ALLOW_ORIGINS: "*"

  # Worker 配置
  AIRFLOW__CELERY__WORKER_CONCURRENCY: 16
  AIRFLOW__CELERY__WORKER_AUTOSCALE: "16,4"
  AIRFLOW__CELERY__WORKER_MAX_MEMORY_PER_CHILD: "512000"
  AIRFLOW__CELERY__OPERATION_TIMEOUT: 1800
  AIRFLOW__CELERY__BROKER_CONNECTION_RETRY_ON_STARTUP: True

  # 系统配置
  TZ: Asia/Shanghai
  PIP_INDEX_URL: https://mirrors.cloud.tencent.com/pypi/simple/
  PIP_TRUSTED_HOST: mirrors.cloud.tencent.com

#===============================================================================
# 服务定义
#===============================================================================
services:
  # 数据库服务
  postgresql:
    <<: *common
    image: bitnami/postgresql:latest
    environment: 
      POSTGRESQL_DATABASE: *db_name
      POSTGRESQL_USERNAME: *db_user
      POSTGRESQL_PASSWORD: *db_pass
    volumes:
      - ./database/postgresql:/bitnami/postgresql
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5

  # Redis 服务
  redis:
    <<: *common
    image: bitnami/redis:latest
    environment:
      ALLOW_EMPTY_PASSWORD: yes
    volumes:
      - ./database/redis:/bitnami
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      retries: 5

  # Airflow Web 服务
  web:
    <<: *airflow-common
    ports:
      - '80:8080'
    environment:
      <<: *airflow-env
      AIRFLOW_COMPONENT_TYPE: webserver
    depends_on:
      postgresql:
        condition: service_healthy
      redis:
        condition: service_healthy

  # Airflow Scheduler 服务
  scheduler:
    <<: *airflow-common
    deploy:
      replicas: 1  # 运行1个调度器实例
    environment:
      <<: *airflow-env
      AIRFLOW_COMPONENT_TYPE: scheduler
    depends_on:
      web:
        condition: service_started

  # Airflow Worker 服务
  worker:
    <<: *airflow-common
    deploy:
      replicas: 1  # 运行1个调度器实例

    ports:
      - "8081:8081"
    volumes:
      - ./dags:/opt/bitnami/airflow/dags
      - ./logs:/opt/bitnami/airflow/logs
      - ./requirements.txt:/bitnami/python/requirements.txt
      - /tmp:/opt/bitnami/airflow/tmp
    # 设置为 root 用户, 用于安装相关系统依赖
    user: root
    command: >
      bash -c "
        sed -i 's/deb.debian.org/mirrors.cloud.tencent.com/g' /etc/apt/sources.list &&
        sed -i 's/security.debian.org/mirrors.cloud.tencent.com/g' /etc/apt/sources.list &&
        apt-get update && 
        apt-get install -y libgl1-mesa-glx libglib2.0-0 libsm6 libxext6 libxrender-dev &&
        chown -R root:root /opt/bitnami &&
        chmod -R 777 /opt/bitnami &&
        /opt/bitnami/scripts/airflow/run.sh
      "
    environment:
      <<: *airflow-env
      AIRFLOW_COMPONENT_TYPE: worker
    depends_on:
      web:
        condition: service_started

  # Airflow DAG Processor 服务
  dag-processor:
    <<: *airflow-common
    volumes:
      - ./dags:/opt/bitnami/airflow/dags
      - ./logs:/opt/bitnami/airflow/logs
      - ./requirements.txt:/bitnami/python/requirements.txt
    environment:
      <<: *airflow-env
      AIRFLOW_COMPONENT_TYPE: dag-processor
    depends_on:
      web:
        condition: service_started

  # Airflow Triggerer 服务
  triggerer:
    <<: *airflow-common
    environment:
      <<: *airflow-env
      AIRFLOW_COMPONENT_TYPE: triggerer
    depends_on:
      web:
        condition: service_started

  # Webhook 服务
  webhook:
    <<: *common
    image: python:3.10
    ports:
      - "5000:5000"
    volumes:
      - ./:/app
      - /root/.ssh:/root/.ssh:ro
    working_dir: /app
    command: >
      bash -c "
        git config --global http.proxy ${PROXY_URL} &&
        git config --global https.proxy ${PROXY_URL} &&
        pip config set global.index-url https://mirrors.cloud.tencent.com/pypi/simple/ &&
        pip config set global.trusted-host mirrors.cloud.tencent.com &&
        pip install --no-cache-dir fastapi uvicorn gunicorn python-dotenv httpx slowapi &&
        gunicorn webhook_server:app --workers 2 --worker-class uvicorn.workers.UvicornWorker --bind 0.0.0.0:5000
      "
    environment:
      - RATE_LIMIT_UPDATE=50/minute
      - RATE_LIMIT_WCF=100/minute
      - TZ=Asia/Shanghai
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
